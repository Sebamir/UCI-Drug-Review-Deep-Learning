{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70849e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, AdamWeightDecay, AutoTokenizer, TrainingArguments, Trainer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset \n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"Dataset/raw/temporal/drugsComTrain_raw.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4f065",
   "metadata": {},
   "source": [
    "Procesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d1450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clasificado = df.query(\"rating >= 7 | rating <= 4\").copy() # Filtramos solo las filas con rating >= 7 o <= 4\n",
    "df_clasificado[\"flag\"] = df_clasificado[\"rating\"].apply(lambda x: 1 if x >= 7 else 0) # Clasificamos los ratings en 1 (positivo) y 0 (negativo)\n",
    "\n",
    "df_clasificado = df_clasificado[[\"review\", \"flag\"]] # Seleccionamos solo las columnas de inter√©s\n",
    "print(df_clasificado.info())# Mostramos la informaci√≥n del DataFrame resultante\n",
    "print(df_clasificado[\"flag\"].value_counts()) # Mostramos la distribuci√≥n de clases\n",
    "print(\"porcentaje de clase positiva: \", df_clasificado[\"flag\"].value_counts(normalize=True)[1] * 100) # Mostramos el porcentaje de clase positiva\n",
    "print(\"porcentaje de clase negativa: \", df_clasificado[\"flag\"].value_counts(normalize=True)[0] * 100) # Mostramos el porcentaje de clase negativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clasificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13f9bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessingDataframe(df, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "\n",
    "    MODEL_NAME = 'distilbert-base-cased'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    # Longitud m√°xima de la secuencia (un est√°ndar para DistilBERT)\n",
    "    MAX_LEN = 256\n",
    "\n",
    "    clase_0 = df[df[\"flag\"] == 0]\n",
    "    clase_1 = df[df[\"flag\"] == 1]\n",
    "    \n",
    "    # reducci√≥n del tama√±o del conjunto de datos para acelerar el entrenamiento\n",
    "\n",
    "    clase_0_subsampled = clase_0.sample(frac=0.5, random_state=42)\n",
    "    clase_1_subsampled = clase_1.sample(frac=0.5, random_state=42)\n",
    "\n",
    "    # duplicamos la clase minoritaria\n",
    "    clase_0_subsampled = pd.concat([clase_0_subsampled, clase_0_subsampled])\n",
    "\n",
    "    df_balanced = pd.concat([clase_0_subsampled, clase_1_subsampled]).sample(frac=1, random_state=42).reset_index(drop=True) # Mezclamos las clases balanceadas  \n",
    "    \n",
    "    text_list = df_balanced['review'].tolist()  \n",
    "    \n",
    "    encoding = tokenizer.batch_encode_plus(\n",
    "    text_list,\n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',     # Rellena secuencias m√°s cortas\n",
    "    truncation=True,          # Corta secuencias m√°s largas\n",
    "    return_attention_mask=True, # Crea la m√°scara (dice d√≥nde est√°n los tokens reales)\n",
    "    return_tensors='pt'       # Devuelve tensores de PyTorch\n",
    "    )\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "\n",
    "    # 3. Preparar Etiquetas (y)\n",
    "    labels = torch.tensor(df_balanced['flag'].values)\n",
    "\n",
    "    # 1. Dividir los datos (80% Entrenamiento, 20% Validaci√≥n)\n",
    "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
    "    input_ids, labels, random_state=42, test_size=0.2, stratify=labels\n",
    "    )\n",
    "    train_masks, validation_masks, _, _ = train_test_split(\n",
    "        attention_mask, input_ids, random_state=42, test_size=0.2, stratify=labels\n",
    "    )\n",
    "\n",
    "    # 1. Combina los tensores en un diccionario\n",
    "    # El Trainer necesita las llaves 'input_ids', 'attention_mask' y 'labels'\n",
    "    train_dict = {\n",
    "        'input_ids': train_inputs.numpy(),\n",
    "        'attention_mask': train_masks.numpy(),\n",
    "        'labels': train_labels.numpy()\n",
    "    }\n",
    "    validation_dict = {\n",
    "        'input_ids': validation_inputs.numpy(),\n",
    "        'attention_mask': validation_masks.numpy(),\n",
    "        'labels': validation_labels.numpy()\n",
    "    }\n",
    "\n",
    "    # 2. Crea el objeto Dataset\n",
    "    train_dataset = Dataset.from_dict(train_dict)\n",
    "    validation_dataset = Dataset.from_dict(validation_dict)\n",
    "\n",
    "    # Correr los c√°lculos de pesos de clase\n",
    "    labels = df_balanced['flag'].values\n",
    "    clase_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(labels),\n",
    "        y=labels\n",
    "        )\n",
    "    weights = torch.tensor(clase_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    return train_dataset, validation_dataset, df_balanced, weights\n",
    "train_dataset, validation_dataset, df_balanced, weights = ProcessingDataframe(df_clasificado)\n",
    "\n",
    "print(df_balanced.info())# Mostramos la informaci√≥n del DataFrame resultante\n",
    "print(df_balanced[\"flag\"].value_counts()) # Mostramos la distribuci√≥n de clases\n",
    "print(\"porcentaje de clase positiva: \", df_balanced[\"flag\"].value_counts(normalize=True)[1] * 100) # Mostramos el porcentaje de clase positiva\n",
    "print(\"porcentaje de clase negativa: \", df_balanced[\"flag\"].value_counts(normalize=True)[0] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef69df",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86203736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calcula las m√©tricas de precisi√≥n (accuracy) y F1-score.\n",
    "    \"\"\"\n",
    "    # predictions son los logits (salida cruda del modelo)\n",
    "    logits, labels = eval_pred \n",
    "    \n",
    "    # Tomar el argmax para obtener la clase predicha (0 o 1)\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Calcular las m√©tricas\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='binary') \n",
    "    precision = precision_score(labels, predictions, average='binary')\n",
    "    recall = recall_score(labels, predictions, average='binary')\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Trainer personalizado que inyecta pesos de clase en la funci√≥n de p√©rdida (CrossEntropyLoss).\n",
    "    \"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False,**kwargs):\n",
    "        # Obtener las etiquetas (y_true)\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "        # Propagaci√≥n hacia adelante (Forward pass)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Crear la funci√≥n de p√©rdida CrossEntropy con los pesos de clase\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights) # \n",
    "        \n",
    "        # Calcular la p√©rdida\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "class Trainer(Trainer):\n",
    "    \"\"\"\n",
    "    Trainer normal sin inyeccion de pesos (CrossEntropyLoss).\n",
    "    \"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False,**kwargs):\n",
    "        # Obtener las etiquetas (y_true)\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "        # Propagaci√≥n hacia adelante (Forward pass)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Crear la funci√≥n de p√©rdida CrossEntropy sin pesos de clase\n",
    "        loss_fct = nn.CrossEntropyLoss() \n",
    "        \n",
    "        # Calcular la p√©rdida\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "def full_training():\n",
    "    MODEL_NAME = 'distilbert-base-cased'\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        num_labels = 2,\n",
    "        output_attentions = False, # No necesitamos las atenciones\n",
    "        output_hidden_states = False, # No necesitamos los estados ocultos\n",
    "    )\n",
    "    \n",
    "    model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu')) # Mover el modelo a CPU (o GPU si est√° disponible)\n",
    "    \n",
    "    # 2. Definir los Argumentos de Entrenamiento (Hyperparameters)\n",
    "    # El 'output_dir' es donde se guardar√°n los checkpoints y logs\n",
    "\n",
    "    training_args1 = TrainingArguments(\n",
    "        output_dir='./results',          # Directorio para guardar outputs\n",
    "        num_train_epochs=2,              # N√∫mero total de √©pocas\n",
    "        per_device_train_batch_size=8,  # Tama√±o del batch por dispositivo (GPU/CPU)\n",
    "        per_device_eval_batch_size=32,   # Tama√±o del batch para evaluaci√≥n\n",
    "        warmup_steps=500,                # N√∫mero de pasos para el warmup del learning rate\n",
    "        weight_decay=0.01,               # Descenso de peso (regularizaci√≥n L2)\n",
    "        logging_dir='./logs',            # Directorio para los logs\n",
    "        logging_steps=50,                # Registrar log cada 50 pasos\n",
    "        fp16=True,               \n",
    "        eval_strategy=\"epoch\",     # Evaluar al final de cada √©poca\n",
    "        save_strategy=\"epoch\",           # Guardar el checkpoint al final de cada √©poca\n",
    "        load_best_model_at_end=True,     # Cargar el mejor modelo despu√©s del entrenamiento\n",
    "        learning_rate=2e-5,          # Tasa de aprendizaje\n",
    "        dataloader_num_workers= 4     # N√∫mero de trabajadores para cargar datos              \n",
    "    )\n",
    "\n",
    "    training_args2 = TrainingArguments(\n",
    "        output_dir='./results',          # Directorio para guardar outputs\n",
    "        num_train_epochs=1,              # N√∫mero total de √©pocas\n",
    "        per_device_train_batch_size=8,  # Tama√±o del batch por dispositivo (GPU/CPU)\n",
    "        per_device_eval_batch_size=32,   # Tama√±o del batch para evaluaci√≥n\n",
    "        warmup_steps=500,                # N√∫mero de pasos para el warmup del learning rate\n",
    "        weight_decay=0.01,               # Descenso de peso (regularizaci√≥n L2)\n",
    "        logging_dir='./logs',            # Directorio para los logs\n",
    "        logging_steps=50,                # Registrar log cada 50 pasos\n",
    "        fp16=True,               \n",
    "        eval_strategy=\"epoch\",     # Evaluar al final de cada √©poca\n",
    "        save_strategy=\"epoch\",           # Guardar el checkpoint al final de cada √©poca\n",
    "        load_best_model_at_end=True,     # Cargar el mejor modelo despu√©s del entrenamiento\n",
    "        learning_rate=2e-5,          # Tasa de aprendizaje\n",
    "        dataloader_num_workers= 4     # N√∫mero de trabajadores para cargar datos              \n",
    "    )\n",
    "    \n",
    "    trainer_1= Trainer(\n",
    "        model=model,                         # El modelo a entrenar\n",
    "        args=training_args1,                  # Los argumentos de entrenamiento\n",
    "        train_dataset=train_dataset,         # Conjunto de datos de entrenamiento\n",
    "        eval_dataset=validation_dataset,     # Conjunto de datos de validaci√≥n\n",
    "        compute_metrics=compute_metrics      # Funci√≥n para calcular m√©tricas\n",
    "    )\n",
    "    \n",
    "    trainer_2 = WeightedTrainer(\n",
    "        model=model,                         # El modelo a entrenar\n",
    "        args=training_args2,                  # Los argumentos de entrenamiento\n",
    "        train_dataset=train_dataset,         # Conjunto de datos de entrenamiento\n",
    "        eval_dataset=validation_dataset,     # Conjunto de datos de validaci√≥n\n",
    "        compute_metrics=compute_metrics      # Funci√≥n para calcular m√©tricas\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    # 3. Entrenar el modelo\n",
    "    trainer_1.train()\n",
    "    # 4. Congelar el modelo y solo entrenar la capa de clasificaci√≥n\n",
    "    for param in model.distilbert.parameters():\n",
    "        param.requires_grad = False # Congelar todas las capas de DistilBERT    \n",
    "\n",
    "    trainer_2.train()\n",
    "    \n",
    "    \n",
    "    return trainer_2\n",
    "\n",
    "print(\"Iniciando el entrenamiento completo con balanceo de clases...\")\n",
    "modelo = full_training()\n",
    "print(\"Guardando el modelo entrenado...\")\n",
    "modelo.save_pretrained(\"Modelo_entrenado/drug_review_classifier_distilbert_fullbalance\") # Guardar el modelo entrenado\n",
    "print(\"Evaluando el modelo entrenado...\")\n",
    "result = modelo.evaluate()\n",
    "print(\"Resultados de la evaluaci√≥n:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86188631",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = 'distilbert-base-cased'\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels = 2,\n",
    "    output_attentions = False, # No necesitamos las matrices de atenci√≥n para la clasificaci√≥n\n",
    "    output_hidden_states = False, # No necesitamos los estados ocultos\n",
    ")\n",
    "\n",
    "model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu')) # Mover el modelo a CPU (o GPU si est√° disponible)\n",
    "\n",
    "# 2. Definir los Argumentos de Entrenamiento (Hyperparameters)\n",
    "# El 'output_dir' es donde se guardar√°n los checkpoints y logs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Directorio para guardar outputs\n",
    "    num_train_epochs=1,              # N√∫mero total de √©pocas\n",
    "    per_device_train_batch_size=8,  # Tama√±o del batch por dispositivo (GPU/CPU)\n",
    "    per_device_eval_batch_size=32,   # Tama√±o del batch para evaluaci√≥n\n",
    "    warmup_steps=500,                # N√∫mero de pasos para el warmup del learning rate\n",
    "    weight_decay=0.01,               # Descenso de peso (regularizaci√≥n L2)\n",
    "    logging_dir='./logs',            # Directorio para los logs\n",
    "    logging_steps=50,                # Registrar log cada 50 pasos\n",
    "    fp16=True,               \n",
    "    eval_strategy=\"epoch\",     # Evaluar al final de cada √©poca\n",
    "    save_strategy=\"epoch\",           # Guardar el checkpoint al final de cada √©poca\n",
    "    load_best_model_at_end=True,     # Cargar el mejor modelo despu√©s del entrenamiento\n",
    "    learning_rate=2e-5,          # Tasa de aprendizaje\n",
    "    dataloader_num_workers= 4     # N√∫mero de trabajadores para cargar datos              \n",
    ")\n",
    "\n",
    "# 3. Crear el Objeto Trainer\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,                         # El modelo DistilBERT\n",
    "    args=training_args,                  # Los argumentos definidos\n",
    "    train_dataset=train_dataset,         # El conjunto de entrenamiento\n",
    "    eval_dataset=validation_dataset,     # El conjunto de validaci√≥n\n",
    "    compute_metrics=compute_metrics,     # La funci√≥n de m√©tricas\n",
    ")\n",
    "\n",
    "# --- Bloque de Verificaci√≥n ---\n",
    "print(\"--- Verificaci√≥n de Dispositivos (GPU) ---\")\n",
    "\n",
    "# Verificar el modelo (debe ser 'cuda:0')\n",
    "print(f\"Modelo DistilBERT en: {next(model.parameters()).device}\")\n",
    "\n",
    "# Verificar los pesos de clase (debe ser 'cuda:0' si la GPU est√° activa)\n",
    "print(f\"Pesos de Clase (weights) en: {weights.device}\")\n",
    "\n",
    "# Verificar el primer lote de datos (Opcional, pero muy informativo)\n",
    "# El Trainer usa un DataLoader para tomar los datos\n",
    "temp_dataloader = trainer.get_train_dataloader()\n",
    "primer_lote = next(iter(temp_dataloader))\n",
    "print(f\"Lote de Datos (input_ids) en: {primer_lote['input_ids'].device}\")\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"Modelo_entrenado/drug_review_classifier_distilbert\") # Guardar el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41419ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"Modelo_entrenado/drug_review_classifier_distilbert\"\n",
    "\n",
    "# 2. Cargar el modelo ENTRENADO (incluye los pesos ajustados y la cabeza de clasificaci√≥n)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# 3. Cargar el tokenizer (necesario para cualquier procesamiento futuro)\n",
    "# Usamos el nombre base para el tokenizer, ya que no cambi√≥.\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "# 4. Mover el modelo al dispositivo (GPU si est√° disponible, CPU si no)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Modelo y Tokenizer cargados y listos para evaluaci√≥n.\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_eval_batch_size=32,   # Usar un batch size de evaluaci√≥n\n",
    "    eval_strategy=\"steps\",     # No importa mucho para una sola llamada a evaluate()\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# 2. Recrear el Objeto Trainer\n",
    "# Si usaste la clase WeightedTrainer, deber√≠as definirla nuevamente aqu√≠.\n",
    "# Usaremos la clase Trainer est√°ndar para simplificar, ya que no estamos entrenando.\n",
    "trainer = Trainer(\n",
    "    model=model,                         # El modelo reci√©n cargado\n",
    "    args=training_args,                  \n",
    "    eval_dataset=validation_dataset,     # üìå Tu conjunto de validaci√≥n debe estar disponible\n",
    "    compute_metrics=compute_metrics,     # üìå Tu funci√≥n de m√©tricas debe estar definida\n",
    ")\n",
    "\n",
    "# 3. Ejecutar la Evaluaci√≥n Final\n",
    "print(\"Iniciando evaluaci√≥n...\")\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# 4. Mostrar el Resultado\n",
    "print(\"\\n--- Resultados Finales ---\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d881b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"Modelo_entrenado/drug_review_classifier_distilbert_FINAL\"\n",
    "model.save_pretrained(MODEL_DIR)\n",
    "\n",
    "# Guarda el tokenizer con su vocabulario\n",
    "tokenizer.save_pretrained(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfc40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå La ruta donde guardaste el modelo y tokenizer\n",
    "MODEL_PATH = \"Modelo_entrenado/drug_review_classifier_distilbert_FINAL\" \n",
    "\n",
    "# 1. Cargar el modelo y tokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "# 2. Definir dispositivo\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "OPTIMAL_THRESHOLD = 0.70 \n",
    "\n",
    "def predict_sentiment_threshold(text, model, tokenizer, device, threshold):\n",
    "    # ... (Pasos 1 y 2: Tokenizar y obtener Logits)\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 3. Obtener probabilidades (Softmax)\n",
    "    probabilities = torch.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Extraer la probabilidad de la clase Positiva (asumiendo que es el √≠ndice 1)\n",
    "    # *Verifica si tu clase Positiva es el √≠ndice 1*\n",
    "    prob_positive = probabilities[0][1].item()\n",
    "    prob_negative = probabilities[0][0].item()\n",
    "    \n",
    "    # 4. Ajustar la decisi√≥n seg√∫n el nuevo umbral:\n",
    "    if prob_positive >= threshold:\n",
    "        predicted_class = 'Positivo'\n",
    "    else:\n",
    "        # Si no es lo suficientemente Positivo (menos del 65%), lo clasificamos como Negativo\n",
    "        predicted_class = 'Negativo'\n",
    "        \n",
    "    return predicted_class, prob_positive, prob_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count_pos = 0\n",
    "count_neg = 0\n",
    "mean = 0 \n",
    "negative_reviews = [\n",
    "    \"La pastilla me caus√≥ una migra√±a horrible y no ayud√≥ en absoluto con mi problema inicial. Pura basura.\",\n",
    "    \"Tuve que dejar de tomarlo a los tres d√≠as. Los efectos secundarios eran peores que la enfermedad. Totalmente inefectivo.\",\n",
    "    \"El producto es in√∫til; no sent√≠ ning√∫n alivio despu√©s de tomarlo durante tres semanas. Sigo igual o peor.\",\n",
    "    \"Me sent√≠ muy decepcionado. El medicamento no cumple lo que promete y solo es una p√©rdida de dinero.\",\n",
    "    \"La farmacia tard√≥ una semana en entregarlo y el servicio al cliente fue grosero. P√©sima experiencia de compra.\",\n",
    "    \"Este fue el peor tratamiento que he probado. El sabor es repugnante y no not√© ninguna mejor√≠a.\",\n",
    "    \"No lo recomiendo en absoluto. Me provoc√≥ n√°useas constantes y tuve que ir al m√©dico para descontinuarlo.\",\n",
    "    \"Esperaba mucho m√°s de esta marca. Claramente fallaron en la formulaci√≥n, ya que no tuve resultados duraderos.\",\n",
    "    \"Compr√© dos cajas y ambas llegaron con el empaque roto. El medicamento es demasiado caro para esta mala calidad.\",\n",
    "    \"No est√° mal, pero es much√≠simo m√°s lento de lo que publicitan. Sent√≠ que no vali√≥ la pena la espera.\"\n",
    "]\n",
    "\n",
    "\n",
    "positive_reviews = [\n",
    "    \"Este medicamento me ha devuelto la calidad de vida que hab√≠a perdido. Es un producto milagroso y lo recomiendo.\",\n",
    "    \"Excelente producto. El efecto se not√≥ en menos de 24 horas y sin efectos secundarios molestos.\",\n",
    "    \"Estoy muy satisfecho con los resultados. Funciona exactamente como se describe en la caja. Un 10/10.\",\n",
    "    \"El alivio que proporciona vale totalmente el precio. Es la mejor inversi√≥n que he hecho en mi salud.\",\n",
    "    \"Fue recetado por mi m√©dico y super√≥ todas mis expectativas. Una maravilla, volver√© a comprarlo sin duda.\",\n",
    "    \"Aunque el env√≠o tard√≥, el resultado final es tan bueno que vale la pena la espera. Funciona a la perfecci√≥n.\",\n",
    "    \"La dosis es f√°cil de seguir y el sabor no es tan malo como otros. Lo m√°s importante: ¬°la mejor√≠a es notable!\",\n",
    "    \"Mi dolor cr√≥nico ha desaparecido casi por completo. Siento una enorme gratitud por este tratamiento.\",\n",
    "    \"Me sorprendi√≥ la eficacia con tan baja concentraci√≥n. Es muy potente y no causa dependencia.\",\n",
    "    \"El servicio al cliente fue genial, y el producto en s√≠ me ayud√≥ mucho m√°s r√°pido de lo que esperaba.\"\n",
    "]\n",
    "\n",
    "OPTIMAL_THRESHOLD = 0.73 \n",
    "for text in positive_reviews:   #positive_reviews o  negative_reviews\n",
    "    sentiment, prob_pos, prob_neg = predict_sentiment_threshold(text, model, tokenizer, device, OPTIMAL_THRESHOLD)\n",
    "    mean += prob_pos / len(neg_text)\n",
    "    if sentiment == 'Positivo':\n",
    "        count_pos += 1\n",
    "    else:\n",
    "        count_neg += 1\n",
    "        \n",
    "    print(f\"Resultado: {sentiment} (Probabilidad_positiva: {prob_pos:.2f}) (Probabilidad_negativa: {prob_neg:.2f})\")\n",
    "print(f\"Positivos: {count_pos}, Negativos: {count_neg}\\n\") \n",
    "print(f\"Mean probabilidad positiva: {mean:.2f}\")\n",
    "print(f\"threhold: {OPTIMAL_THRESHOLD}\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
